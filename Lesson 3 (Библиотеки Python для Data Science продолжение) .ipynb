{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e28a6720",
   "metadata": {},
   "source": [
    "### Урок 3. Построение модели классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234bd744",
   "metadata": {},
   "source": [
    "### \n",
    "1. Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84806fc2",
   "metadata": {},
   "source": [
    "### \n",
    "Метрики классификации определены для двоичной классификации в двух классах и требуют усреднения по классам для получения одной оценки для многоклассовой классификации. Scikit-learn предоставляет несколько методов усреднения, три из которых раскрываются при автоматизированном ML: micro, macro, weighted.\n",
    "\n",
    "'micro': рассчитывает значение метрик исходя из суммарного числа true positives,\n",
    "false negatives и false positives предсказаний.\n",
    "\n",
    "'macro': рассчитывает метрики для каждого из классов и вычисляет невзвешенное\n",
    "среднее. Дисбаланс классов не учитывается.\n",
    "\n",
    "'weighted': рассчитывает метрики для каждого из классов и вычисляет взвешенное\n",
    "среднее с учётом дисбаланса классов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c261c619",
   "metadata": {},
   "source": [
    "### \n",
    "2. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5ed21",
   "metadata": {},
   "source": [
    "### \n",
    "Алгоритмы отличаются друг от друга реализацией алгоритма усиленных деревьев, их технической совместимостью и ограничениями. XGBoost был первым, кто попытался улучшить время обучения GBM, за ним последовали LightGBM и CatBoost, каждый со своими собственными методами, в основном связанными с механизмом разделения. Пакеты всех алгоритмов постоянно пополняются новыми функциями и возможностями. В большинстве случаев мы представляем поведение алгоритмов по умолчанию в R, хотя могут быть доступны и другие параметры. В этом разделе мы сравниваем только XGBoost, LightGBM и CatBoost. Если вы пропустили GBM, не волнуйтесь - вы найдете его в разделе результатов.\n",
    "\n",
    "XGBoost\n",
    "• Стандартный градиентный бустинг\n",
    "• Стохастический градиентный бустинг\n",
    "• Регуляризованный градиентный бустинг с L1 и L2 регуляризацией.\n",
    "Особенности алгоритма:\n",
    "• Различные стратегии обработки пропущенных данных.\n",
    "• Блочная структура для поддержки распараллеливания обучения деревьев.\n",
    "• Продолжение обучения для дообучения на новых данных.\n",
    "Параметры: • n_estimators — число деревьев. • eta — размер шага. Предотвращает переобучение. • gamma — минимальное изменение значения loss функции для разделения листа на поддеревья. • max_depth — максимальная глубина дерева. • lambda/alpha — L2/L1 регуляризация.\n",
    "\n",
    "LightGBM\n",
    "• Градиентная односторонняя выборка (GOSS)\n",
    "• Exclusive Feature Bundling (объединение взаимоисключающих признаков, EFB)\n",
    "Параметры:\n",
    "• num_leaves\n",
    "• min_data_in_leaf\n",
    "• max_depth.\n",
    "\n",
    "CatBoost\n",
    "Процесс построения происходит жадно.\n",
    "• Выбираем первую вершину;\n",
    "• Выбираем лучшее дерево с одной вершиной;\n",
    "• Считаем метрику и по ней выбираем лучшее дерево.\n",
    "Параметры: • cat_features; • Overfitting detector; • Число итераций и learning rate; • L2_reg; • Random_srength; • Bagging_temp; • Глубина дерева (стоит попробовать 10 и 6)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
